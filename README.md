# Welcome to the repository of the [Website](https://prod---website-vbip4gqz2q-uc.a.run.app) that shows the paper's results

[![Website](https://img.shields.io/website?up_color=green&url=https%3A%2F%2Fprod---website-vbip4gqz2q-uc.a.run.app%2F)](https://prod---website-vbip4gqz2q-uc.a.run.app/)
[![Super linter](https://github.com/HMS-Internship/Website/actions/workflows/linter.yml/badge.svg)](https://github.com/HMS-Internship/Website/actions/workflows/linter.yml)

The website is coded in Python, using the framework Dash. The data is stored on AWS s3. We use Cloud Run from Google Cloud Platform to host our website.

## Contribute to the project

The fact that you cannot have access to the data stored on AWS (since we don't share the credentials) makes it harder to contribute to the project. However, you can still propose the some changes with a pull request.

Once you have forked the repository and cloned it, you can install the package with its development dependencies using:
```bash
pip install -e .[env]
```

The command `launch_local_website` allows you to test the website locally.

If you are using Visual Studio Code, a [.devcontainer](/.devcontainer) folder is already prepared so that you can work in a dedicated container.

Feel free to discuss about you ideas in the [discussion section](https://github.com/HMS-Internship/Website/discussions).

## Structure of website's code
```bash
ğŸ“¦Website
 â”£ ğŸ“‚.devcontainer
 â”ƒ â”£ ğŸ“œDockerfile
 â”ƒ â”— ğŸ“œdevcontainer.json
 â”£ ğŸ“‚post_processing
 â”ƒ â”£ ğŸ“‚all_categories
 â”ƒ â”ƒ â”£ ğŸ“œfeature_importances_correlation.py
 â”ƒ â”ƒ â”£ ğŸ“œinformation.py
 â”ƒ â”ƒ â”£ ğŸ“œscores_feature_importances.py
 â”ƒ â”ƒ â”— ğŸ“œscores_residual.py
 â”ƒ â”£ ğŸ“‚custom_categories
 â”ƒ â”ƒ â”£ ğŸ“œcreate_custom_data.py
 â”ƒ â”ƒ â”— ğŸ“œselect_categories.py
 â”ƒ â”— ğŸ“œ__init__.py
 â”£ ğŸ“‚website
 â”ƒ â”£ ğŸ“‚dataset
 â”ƒ â”ƒ â”— ğŸ“œpage.py
 â”ƒ â”£ ğŸ“‚feature_importances
 â”ƒ â”ƒ â”— ğŸ“œpage.py
 â”ƒ â”£ ğŸ“‚feature_importances_correlations
 â”ƒ â”ƒ â”£ ğŸ“‚between_algorithms
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚tabs
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œall_categories.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcustom_categories.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œshared_plotter.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œpage.py
 â”ƒ â”ƒ â”— ğŸ“‚between_targets
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚tabs
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œall_categories.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcustom_categories.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œshared_plotter.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œpage.py
 â”ƒ â”£ ğŸ“‚prediction_performances
 â”ƒ â”ƒ â”£ ğŸ“‚feature_importances
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚tabs
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œall_categories.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcustom_categories.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œshared_plotter.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œpage.py
 â”ƒ â”ƒ â”— ğŸ“‚residual
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚tabs
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œall_categories.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcustom_categories.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œshared_plotter.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œpage.py
 â”ƒ â”£ ğŸ“‚residual_correlations
 â”ƒ â”ƒ â”£ ğŸ“‚tabs
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œall_categories.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcustom_categories.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œshare_plotter.py
 â”ƒ â”ƒ â”— ğŸ“œpage.py
 â”ƒ â”£ ğŸ“‚utils
 â”ƒ â”ƒ â”£ ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“œaws_loader.py
 â”ƒ â”ƒ â”£ ğŸ“œcontrols.py
 â”ƒ â”ƒ â”£ ğŸ“œgraphs.py
 â”ƒ â”ƒ â”— ğŸ“œrename.py
 â”ƒ â”£ ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“œapp.py
 â”ƒ â”£ ğŸ“œindex.py
 â”ƒ â”— ğŸ“œintroduction.py
 â”£ ğŸ“œ.dockerignore
 â”£ ğŸ“œ.gitignore
 â”£ ğŸ“œDockerfile
 â”£ ğŸ“œLICENSE
 â”£ ğŸ“œREADME.md
 â”— ğŸ“œsetup.py
```

## Structure of the website's data (on AWS s3)
```Bash
ğŸ“¦age-vs-survival
 â”£ ğŸ“‚all_categories
 â”ƒ â”£ ğŸ“‚correlations
 â”ƒ â”ƒ â”£ ğŸ“‚feature_importances
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpearson_between_algorithms.feather
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpearson_between_targets.feather
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpearson_[...].feather  # Original data not used in website
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œspearman_between_algorithms.feather
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œspearman_between_targets.feather
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œspearman_[...].feather  # Original data not used in website  # Original data not used in website
 â”ƒ â”ƒ â”— ğŸ“‚residual
 â”ƒ â”ƒ   â”£ ğŸ“œnumber_participants_[...].feather
 â”ƒ â”ƒ   â”£ ğŸ“œpearson_[...].feather
 â”ƒ â”ƒ   â”— ğŸ“œspearman_[...].feather
 â”ƒ â”£ ğŸ“œinformation.feather
 â”ƒ â”£ ğŸ“œscores_feature_importances.feather
 â”ƒ â”— ğŸ“œscores_residual.feather
 â”£ ğŸ“‚custom_categories
 â”ƒ â”£ ğŸ“‚correlations
 â”ƒ â”ƒ â”£ ğŸ“‚feature_importances
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpearson_between_algorithms.feather
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpearson_between_targets.feather
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œspearman_between_algorithms.feather
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œspearman_between_targets.feather
 â”ƒ â”ƒ â”— ğŸ“‚residual
 â”ƒ â”ƒ   â”£ ğŸ“œnumber_participants_[...].feather
 â”ƒ â”ƒ   â”£ ğŸ“œpearson_[...].feather
 â”ƒ â”ƒ   â”— ğŸ“œspearman_[...].feather
 â”ƒ â”£ ğŸ“œinformation.feather
 â”ƒ â”£ ğŸ“œscores_feature_importances.feather
 â”ƒ â”— ğŸ“œscores_residual.feather
 â”£ ğŸ“‚examination
 â”ƒ â”— ğŸ“œcategory.feather
 â”£ ğŸ“‚feature_importances
 â”ƒ â”£ ğŸ“‚examination
 â”ƒ â”ƒ â”— ğŸ“œcategory.feather
 â”ƒ â”£ ğŸ“‚laboratory
 â”ƒ â”ƒ â”— ğŸ“œcategory.feather
 â”ƒ â”— ğŸ“‚questionnaire
 â”ƒ   â”— ğŸ“œcategory.feather
 â”£ ğŸ“‚laboratory
 â”ƒ â”— ğŸ“œcategory.feather
 â”£ ğŸ“‚questionnaire
 â”ƒ â”— ğŸ“œcategory.feather
 â”£ ğŸ“œResults.xlsx  # Original data not used in website
 â”— ğŸ“œfavicon.ico
```

## How to deploy

A CI / CD workflow has been created with Git Actions in order to deploy the website automatically on demand. You can find the development version of the website [here](https://dev---website-vbip4gqz2q-uc.a.run.app/).

## How to update the data

Under the folder __data__, the files that can be updated are the following:
- Results.xlsx
- correlations/feature_importances/correlation_type_main_category.feather
- correlations/feature_importances/correlation_type_std_main_category.feather
- correlations/residual/number_participants_type_of_death_type_of_death.feather
- correlations/residual/correlation_type_type_of_death_type_of_death.feather
- main_category/category.feather

To generate the other files, all the scripts under the folder __data__ has to be executed.
The file [select_categories.py](./post_processing/custom_categories/select_categories.py) outputs the 30 best models. This list has to match with the list called *CUSTOM_CATEGORIES_INDEX* in the [website code](./website/__init__.py).

Once everything is done, don't forget to push the new \_\_init__.py file to GitHub, to push the new data the AWS s3 so that you can use the CI / CD pipeline.

To sync the __data__ folder to AWS s3, you can use the following:
```bash
aws s3 sync data/ age-vs-survival/ --delete
```